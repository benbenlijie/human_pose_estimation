{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"e4cb1f60-355d-41cb-b309-67b58b0f93cc\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      document.getElementById(\"e4cb1f60-355d-41cb-b309-67b58b0f93cc\").textContent = \"BokehJS successfully loaded.\";\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"e4cb1f60-355d-41cb-b309-67b58b0f93cc\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'e4cb1f60-355d-41cb-b309-67b58b0f93cc' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"e4cb1f60-355d-41cb-b309-67b58b0f93cc\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"e4cb1f60-355d-41cb-b309-67b58b0f93cc\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, batch_size, image_shape, stack_size=2, module_size=2, channel_size=[32, 64, 128]):\n",
    "        self.batch_size = batch_size\n",
    "        self.image_shape = image_shape\n",
    "        self.stack_size = stack_size\n",
    "        self.module_size = module_size\n",
    "        self.channel_size = channel_size\n",
    "        self.alpha = 0.2\n",
    "        #with tf.variable_scope(\"generator\"):\n",
    "        self.inputs = tf.placeholder(tf.float32, shape=[None, *image_shape], name=\"inputs\")\n",
    "    \n",
    "    def leaky_relu(self, inputs):\n",
    "        return tf.maximum(inputs, inputs * self.alpha)\n",
    "    \n",
    "    def kinit(size, dtype, partition_info):\n",
    "        return tf.random_normal(size, stddev=0.02)\n",
    "\n",
    "    def build_module(self, inputs, name, channels=[32, 64, 128]):\n",
    "        with tf.variable_scope(name + \"_module\"):\n",
    "            with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], padding=\"SAME\", kernel_size=3, stride=2, activation_fn=self.leaky_relu, \n",
    "                          weights_initializer=tf.truncated_normal_initializer(stddev=0.01)):\n",
    "                return self.conv_module(inputs, channels)\n",
    "\n",
    "    def conv_module(self, inputs, channels):\n",
    "        conv_layer = slim.conv2d(inputs, channels[0])\n",
    "        if len(channels) > 1:\n",
    "            #recursive\n",
    "            inner_layer = self.conv_module(conv_layer, channels[1:])\n",
    "            concat_layer = tf.concat([conv_layer, inner_layer], axis=-1)\n",
    "        else:\n",
    "            concat_layer = conv_layer\n",
    "        deconv_layer = slim.conv2d_transpose(concat_layer, channels[0])\n",
    "        return deconv_layer\n",
    "    \n",
    "    def build_stack(self, inputs, training=True):\n",
    "        with slim.arg_scope([slim.conv2d], kernel_size=5, stride=1, padding=\"SAME\",\n",
    "                       activation_fn=self.leaky_relu, weights_initializer=tf.truncated_normal_initializer(stddev=0.02)):\n",
    "            conv_layer = slim.conv2d(inputs, num_outputs=32, )\n",
    "            pose_heatmap = self.build_module(conv_layer, \"pose\", self.channel_size)\n",
    "            concat_layer = tf.concat([pose_heatmap, conv_layer], axis=-1)\n",
    "            occlusion_heatmap = self.build_module(concat_layer, \"occlusion\", self.channel_size)\n",
    "            pose_heatmap = slim.conv2d(pose_heatmap, 1)\n",
    "            occlusion_heatmap = slim.conv2d(occlusion_heatmap, 1)\n",
    "            pose_heatmap = tf.nn.tanh(pose_heatmap)\n",
    "            occlusion_heatmap = tf.nn.tanh(occlusion_heatmap)\n",
    "            output = tf.concat([conv_layer, pose_heatmap, occlusion_heatmap], axis=-1, name=\"stack_output\")\n",
    "            return output, pose_heatmap, occlusion_heatmap\n",
    "    \n",
    "    def build(self, training=True):\n",
    "        module_input = self.inputs\n",
    "        for i in range(self.module_size):\n",
    "            with tf.variable_scope(\"generator_\"+str(i), reuse=not training):\n",
    "                module_output, pose_heatmap, occlusion_heatmap = self.build_stack(module_input, training)\n",
    "                module_input = module_output\n",
    "        self.pose_heatmap = pose_heatmap\n",
    "        self.occlusion_heatmap = occlusion_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_img(fileName, new_size=[512, 512]):\n",
    "    filePattern = \"./sample_img/{}.jpg\"\n",
    "    fileSource = filePattern.format(fileName)\n",
    "    avatar = Image.open(fileSource)\n",
    "    return avatar.resize(new_size, Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img = \"4e497571583a077564df4b547e40408fd9915ecc\"\n",
    "\n",
    "tmp_img = resize_img(test_img)\n",
    "tmp_np_img = np.array(tmp_img)\n",
    "tmp_np_img.shape\n",
    "tmp_img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_path = \"./model_res/keypoint_annotation.pkl\"\n",
    "\n",
    "with open(save_path, mode=\"rb\") as f:\n",
    "    kp_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, image_folder, kp_data, batch_size=32, image_size=[512, 512]):\n",
    "        self.image_folder = image_folder\n",
    "        self.file_pattern = image_folder+\"/{}.jpg\"\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.image_names = [os.path.splitext(file)[0] for file in os.listdir(image_folder)]\n",
    "        self.kp_data = kp_data\n",
    "    \n",
    "    def get_batch(self):\n",
    "        if self.image_names == None:\n",
    "            return None\n",
    "        total_amount = len(self.image_names)\n",
    "        batch_count = total_amount // self.batch_size\n",
    "        for i in range(0, batch_count * self.batch_size, self.batch_size):\n",
    "            the_batch = self.image_names[i:i+self.batch_size]\n",
    "            heatmaps = np.array(list(map(self.make_heatmap, the_batch)))\n",
    "            yield np.array(list(map(self.preprocess, the_batch))), heatmaps[:, 0], heatmaps[:, 1]\n",
    "        pass\n",
    "    \n",
    "    def preprocess(self, image_name):\n",
    "        file_path = self.file_pattern.format(image_name)\n",
    "        avatar = Image.open(file_path)\n",
    "\n",
    "        resized_image = avatar.resize(self.image_size, Image.ANTIALIAS)\n",
    "        return np.array(resized_image) / 255.\n",
    "        \n",
    "    \n",
    "    def get_kp_data(self, image_name):\n",
    "        human_kp_data = self.kp_data[image_name]['keypoint_annotations']['human1']\n",
    "        return human_kp_data\n",
    "    \n",
    "    def make_heatmap(self, image_name):\n",
    "        file_path = self.file_pattern.format(image_name)\n",
    "        avatar = Image.open(file_path)\n",
    "        width, height = avatar.size\n",
    "        kp_data = self.get_kp_data(image_name)\n",
    "        heatmap = np.ones((self.image_size[1], self.image_size[0], 1)) * -1\n",
    "        occlusion_heatmap = np.ones((self.image_size[1], self.image_size[0], 1)) * -1\n",
    "        for i in range(0, len(kp_data), 3):\n",
    "            ori_x, ori_y, status = kp_data[i:i+3]\n",
    "            new_x = int(ori_x * (self.image_size[0] * 1. / width))\n",
    "            new_y = int(ori_y * (self.image_size[1] * 1. / height))\n",
    "            if status == 1:\n",
    "                heatmap[new_y, new_x, 0] = 1\n",
    "            elif status == 2:\n",
    "                occlusion_heatmap[new_y, new_x, 0] = 1\n",
    "        return heatmap, occlusion_heatmap\n",
    "    \n",
    "    def DrawImage(self, image_name):\n",
    "        file_path = self.file_pattern.format(image_name)\n",
    "        avatar = Image.open(file_path)\n",
    "        print(avatar.size)\n",
    "        drawAvatar = ImageDraw.Draw(avatar)\n",
    "        annotation_data = self.kp_data[image_name]\n",
    "        draw_human_boundary(drawAvatar, annotation_data)\n",
    "        draw_keypoint(drawAvatar, annotation_data)\n",
    "        del drawAvatar\n",
    "        return avatar\n",
    "\n",
    "    def draw_human_boundary(drawAvatar, annotation_data):\n",
    "        if 'human_annotations' in annotation_data:\n",
    "            human_data_set = annotation_data['human_annotations']\n",
    "            for humman_data in human_data_set:\n",
    "                drawAvatar.rectangle(human_data_set[humman_data], outline=(255, 10, 0))\n",
    "\n",
    "    def draw_keypoint(drawAvatar, annotation_data):\n",
    "        if \"keypoint_annotations\" in annotation_data:\n",
    "            keypoint_data_set = annotation_data[\"keypoint_annotations\"]\n",
    "            for keypoint_key in keypoint_data_set:\n",
    "                points = keypoint_data_set[keypoint_key]\n",
    "                for i in range(0, len(points), 3):\n",
    "                    if points[i+2] == 1:\n",
    "                        fill = (10, 255, 10)\n",
    "                    elif points[i+2] == 2:\n",
    "                        fill = (255, 10, 10)\n",
    "                    else:\n",
    "                        fill = None\n",
    "                    if fill is not None:\n",
    "                        arc_points = [points[i] - 3, points[i+1] - 3, points[i] + 3, points[i+1] + 3]\n",
    "                        drawAvatar.arc(arc_points, start=0, end=360, fill=fill)\n",
    "                        drawAvatar.text((points[i]+10, points[i+1]), \"{}\".format((i // 3)+1), fill=fill)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preprocessor.get_kp_data(test_img)\\n\\nheatmap, occlusion_heatmap = preprocessor.make_heatmap(test_img)\\n\\nImage.fromarray(((heatmap + 1) * 255).squeeze().astype(np.int32))\\n\\n\\nheatmap[((heatmap + 1) * 255).squeeze().astype(np.int32).nonzero()]\\n\\ntest_img = \"1b75657cd05ff89859bf800a30c0691c776dd880\"\\npreprocessor.DrawImage(test_img)\\n\\n\\na, b, c = preprocessor.image_names[0:3]\\nprint(a, b, c)\\n\\nlen(preprocessor.image_names)\\nlist(map(lambda x: x[-1], preprocessor.image_names))\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"preprocessor.get_kp_data(test_img)\n",
    "\n",
    "heatmap, occlusion_heatmap = preprocessor.make_heatmap(test_img)\n",
    "\n",
    "Image.fromarray(((heatmap + 1) * 255).squeeze().astype(np.int32))\n",
    "\n",
    "\n",
    "heatmap[((heatmap + 1) * 255).squeeze().astype(np.int32).nonzero()]\n",
    "\n",
    "test_img = \"1b75657cd05ff89859bf800a30c0691c776dd880\"\n",
    "preprocessor.DrawImage(test_img)\n",
    "\n",
    "\n",
    "a, b, c = preprocessor.image_names[0:3]\n",
    "print(a, b, c)\n",
    "\n",
    "len(preprocessor.image_names)\n",
    "list(map(lambda x: x[-1], preprocessor.image_names))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Discriminator:\n",
    "    def __init__(self, batch_size, image_shape, output_size=14, channel_size=[32, 64, 128]):\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.channel_size = channel_size\n",
    "        self.output_size = output_size\n",
    "        self.alpha = 0.2\n",
    "\n",
    "    def leaky_relu(self, inputs):\n",
    "        return tf.maximum(inputs, inputs * self.alpha)\n",
    "    \n",
    "    def kinit(size, dtype, partition_info):\n",
    "        return tf.random_normal(size, stddev=0.02)\n",
    "    \n",
    "    def build_module(self, inputs, name, channels=[32, 64, 128]):\n",
    "        with tf.variable_scope(name + \"_module\"):\n",
    "            with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], padding=\"SAME\", kernel_size=3, stride=2, activation_fn=self.leaky_relu, \n",
    "                          weights_initializer=tf.truncated_normal_initializer(stddev=0.01)):\n",
    "                return self.conv_module(inputs, channels)\n",
    "\n",
    "    def conv_module(self, inputs, channels):\n",
    "        conv_layer = slim.conv2d(inputs, channels[0])\n",
    "        if len(channels) > 1:\n",
    "            #recursive\n",
    "            inner_layer = self.conv_module(conv_layer, channels[1:])\n",
    "            concat_layer = tf.concat([conv_layer, inner_layer], axis=-1)\n",
    "        else:\n",
    "            concat_layer = conv_layer\n",
    "        deconv_layer = slim.conv2d_transpose(concat_layer, channels[0])\n",
    "        return deconv_layer\n",
    "    \n",
    "    def build(self, inputs, name, reuse=False):\n",
    "        with tf.variable_scope(\"discriminator_\"+name, reuse=reuse):\n",
    "            conv_output = self.build_module(inputs, name, self.channel_size)\n",
    "            flatten_layer = slim.flatten(conv_output)\n",
    "            self.digits = slim.fully_connected(flatten_layer, self.output_size, activation_fn=None, \n",
    "                                 weights_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "            \n",
    "            self.outputs = tf.nn.sigmoid(self.digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size= 2\n",
    "image_size = [512, 512, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#build generator model\n",
    "generator = Generator(batch_size, image_size)\n",
    "generator.build(True)\n",
    "#get generator output\n",
    "pose_heatmap = generator.pose_heatmap\n",
    "occlusion_heatmap = generator.occlusion_heatmap\n",
    "\n",
    "#create discriminator\n",
    "pose_discriminator = Discriminator(batch_size, image_size)\n",
    "conf_discriminator = Discriminator(batch_size, image_size)\n",
    "\n",
    "#true heatmap input\n",
    "pose_pl = tf.placeholder(tf.float32, [None, image_size[0], image_size[1], 1], name=\"pose_true_heatmap\")\n",
    "occlusion_pl = tf.placeholder(tf.float32, [None, image_size[0], image_size[1], 1], name=\"occlusion_true_heatmap\")\n",
    "\n",
    "#discriminator for true heatmap\n",
    "pose_true_inputs = tf.concat([generator.inputs, pose_pl, occlusion_pl], axis=-1,\n",
    "                            name=\"pose_true_inputs\")\n",
    "conf_true_inputs = tf.concat([pose_pl, occlusion_pl], axis=-1,\n",
    "                            name=\"conf_true_inputs\")\n",
    "pose_discriminator.build(pose_true_inputs, \"pose\")\n",
    "conf_discriminator.build(conf_true_inputs, \"confidence\")\n",
    "pose_true_output = [pose_discriminator.digits, pose_discriminator.outputs]\n",
    "conf_true_output = [conf_discriminator.digits, conf_discriminator.outputs]\n",
    "\n",
    "#discriminator for fake heatmap\n",
    "pose_fake_inputs = tf.concat([generator.inputs, generator.pose_heatmap, generator.occlusion_heatmap], axis=-1,\n",
    "                            name=\"pose_fake_inputs\")\n",
    "conf_fake_inputs = tf.concat([generator.pose_heatmap, generator.occlusion_heatmap], axis=-1,\n",
    "                            name=\"conf_fake_inputs\")\n",
    "pose_discriminator.build(pose_fake_inputs, \"pose\", True)\n",
    "conf_discriminator.build(conf_fake_inputs, \"confidence\", True)\n",
    "pose_fake_output = [pose_discriminator.digits, pose_discriminator.outputs]\n",
    "conf_fake_output = [conf_discriminator.digits, conf_discriminator.outputs]\n",
    "\n",
    "variables = tf.trainable_variables()\n",
    "slim.summarize_tensors(variables)\n",
    "merged = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lam = 10\n",
    "pose_eps = tf.random_uniform([image_size[0], image_size[1], 1], minval=0., maxval=1.)\n",
    "pose_inter = pose_eps * pose_pl + (1. - pose_eps) * generator.pose_heatmap\n",
    "occlusion_eps = tf.random_uniform([image_size[0], image_size[1], 1], minval=0., maxval=1.)\n",
    "occlusion_inter = occlusion_eps * occlusion_pl + (1. - occlusion_eps) * generator.occlusion_heatmap\n",
    "\n",
    "pose_inter_input = tf.concat([generator.inputs, pose_inter, occlusion_inter], axis=-1,\n",
    "                            name=\"pose_fake_inputs\")\n",
    "pose_discriminator.build(pose_inter_input, \"pose\", True)\n",
    "pose_grad = tf.gradients(pose_discriminator.digits, [pose_inter_input])[0]\n",
    "pose_grad_norm = tf.sqrt(tf.reduce_sum((pose_grad)**2, axis=1))\n",
    "pose_grad_pen = lam * tf.reduce_mean((pose_grad_norm - 1)**2)\n",
    "D_pose_loss = tf.reduce_mean(pose_fake_output[0]) - tf.reduce_mean(pose_true_output[0]) + pose_grad_pen\n",
    "\n",
    "\n",
    "conf_inter_input = tf.concat([pose_inter, occlusion_inter], axis=-1,\n",
    "                            name=\"conf_fake_inputs\")\n",
    "conf_discriminator.build(conf_inter_input, \"confidence\", True)\n",
    "conf_grad = tf.gradients(conf_discriminator.digits, [conf_inter_input])[0]\n",
    "conf_grad_norm = tf.sqrt(tf.reduce_sum((conf_grad)**2, axis=1))\n",
    "conf_grad_pen = lam * tf.reduce_mean((conf_grad_norm - 1)**2)\n",
    "D_conf_loss = tf.reduce_mean(conf_fake_output[0]) - tf.reduce_mean(conf_true_output[0]) + conf_grad_pen\n",
    "\n",
    "G_loss = -tf.reduce_mean(pose_fake_output[0]) - tf.reduce_mean(conf_fake_output[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_variables = tf.trainable_variables()\n",
    "generate_vars = [var for var in all_variables if var.name.startswith(\"generator\")]\n",
    "pose_d_vars = [var for var in all_variables if var.name.startswith(\"discriminator_pose\")]\n",
    "conf_d_vars = [var for var in all_variables if var.name.startswith(\"discriminator_conf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pose_d_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer(0.001, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "beta1 = 0.2\n",
    "preprocessor = Preprocessor(\"sample_img\", kp_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_pose_solver = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(D_pose_loss, var_list=pose_d_vars)\n",
    "D_conf_solver = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(D_conf_loss, var_list=conf_d_vars)\n",
    "G_solver = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(G_loss, var_list=generate_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    fileWriter = tf.summary.FileWriter(\"summary\", sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i, (each_batch, pose_heatmap, occlusion_heatmap) in enumerate(preprocessor.get_batch()):\n",
    "        print(each_batch.shape, pose_heatmap.shape, occlusion_heatmap.shape)\n",
    "\n",
    "        feed_dict={\n",
    "                generator.inputs:each_batch,\n",
    "                pose_pl:pose_heatmap,\n",
    "                occlusion_pl:occlusion_heatmap\n",
    "            }\n",
    "        \n",
    "        d_pose_loss, _ = sess.run([D_pose_loss, D_pose_solver], feed_dict=feed_dict)\n",
    "        d_conf_loss, _ = sess.run([D_conf_loss, D_conf_solver], feed_dict=feed_dict)\n",
    "        g_loss, _ = sess.run([G_loss, G_solver], feed_dict=feed_dict)\n",
    "        summary = sess.run(merged, feed_dict=feed_dict)\n",
    "        print(\"pose loss:{}\".format(d_pose_loss),\n",
    "             \"conf loss:{}\".format(d_conf_loss),\n",
    "             \"generate loss:{}\".format(g_loss))\n",
    "        fileWriter.add_summary(summary, i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
